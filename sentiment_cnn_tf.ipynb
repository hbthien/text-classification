{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/dennybritz/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from urllib.request import urlopen\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_data_and_labels(pos_link, neg_link):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "#     # Pull sentences with positive sentiment\n",
    "    pos_file = urlopen(pos_link)\n",
    " \n",
    "    # Pull sentences with negative sentiment\n",
    "    neg_file = urlopen(neg_link)\n",
    "\n",
    "    # Load data from files\n",
    "    positive_examples = list(pos_file.readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(neg_file.readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    \n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent.decode('latin1')) for sent in x_text] \n",
    "    # or:  x_text = [clean_str(str(sent)) for sent in x_text]\n",
    "\n",
    "    \n",
    "    # Generate labels (with two types)\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    \n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1 #nb of iterations per epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index] #return a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (?, 1, 1, 128) (?, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Word_CNN_Model:\n",
    "    \"\"\"Word-level CNN model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=32, num_classes=2, vocab_size=59, embedding_size=128, \n",
    "            filter_sizes=[7], num_filters=128, l2_reg_lambda=0.0, dtype=tf.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "            Inputs:\n",
    "            - input_dim: Tuple (H, W, C) giving size of input data\n",
    "            - num_classes: Number of scores to produce from the final affine layer\n",
    "            - vocab_size: Number of vocabs\n",
    "            - embedding_size: size of embedded text\n",
    "            - num_filters: Number of filters to use in the convolutional layer\n",
    "            - filter_size: Size of filters to use in the convolutional layer\n",
    "            - weight_scale: Scalar giving standard deviation for random initialization\n",
    "              of weights.\n",
    "            - l2_reg_lambda: Scalar giving L2 regularization strength\n",
    "            - dtype: tf datatype to use for computation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_x = tf.placeholder(dtype=tf.int32, shape=[None, input_dim], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='dropout')\n",
    "        \n",
    "        l2_loss = tf.constant(0.0) #l2 regularization loss (optional)\n",
    " \n",
    "        #Embedding layer\n",
    "        with tf.device(\"/cpu:0\"), tf.name_scope(\"embedding\"):\n",
    "            gen = tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)\n",
    "            self.W = tf.Variable (gen, name =\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1) #expand one more dimension\n",
    "            \n",
    "        # Create a set of blocks\n",
    "        pooled_outputs = []\n",
    "        for i, filtersize in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv_maxpool_block_%s\" %filtersize) as main_scope: \n",
    "\n",
    "                    ###conv layer\n",
    "    #                 #conv+relu\n",
    "    #                 conv = tf.layers.conv2d(inputs=self.embedded_chars_expanded, filters=num_filters, \n",
    "    #                                         kernel_size=(filtersize, embedding_size), \n",
    "    # #                                         kernel_initializer= tf.truncated_normal(shape=[filtersize, embedding_size], stddev=0.1),\n",
    "    #                                         strides=(1,1), padding='valid', activation='relu',\n",
    "    # #                                         bias_initializer=tf.constant(0.1, shape=[num_filters]), \n",
    "    #                                         name=str(\"conv_%d\"%i) )\n",
    "    #                 #pool\n",
    "    #                 pooled = tf.layers.max_pooling2d(conv, pool_size=(1, sequence_length - filter_size + 1),\n",
    "    #                                                  strides=(1,1), padding='valid', name=str('pool_%d'%i))\n",
    "\n",
    "                    #define parameters\n",
    "                    ran = tf.truncated_normal(shape=[filtersize, embedding_size, 1, num_filters], stddev=0.1)\n",
    "                    W = tf.Variable(ran, name='W')\n",
    "#                     W = tf.get_variable(\"W\", initializer=ran)\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b_%d\" %i)\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        input=self.embedded_chars_expanded,\n",
    "                        filter=W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=str(\"conv%d\"%i))\n",
    "                    # relu\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=str(\"relu%d\"%i))\n",
    "                    # maxpool\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, input_dim - filtersize + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=str(\"pool%d\"%i))\n",
    "\n",
    "                    pooled_outputs.append(pooled)\n",
    "\n",
    "        # combine all pools\n",
    "        h_pool = tf.concat(pooled_outputs, 3) #Concatenates tensors along  dimension 3\n",
    "        self.h_pool = tf.reshape(h_pool, [-1, num_filters*len(filter_sizes)]) #flatten\n",
    "        print(len(pooled_outputs), h_pool.get_shape(), self.h_pool.get_shape()) #1 (?, 1, 1, 128) (?, 128)\n",
    "        \n",
    "        #dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool, keep_prob=self.keep_prob)\n",
    "            \n",
    "        # Final (unnormalized) scores and predictions\n",
    "#         with tf.name_scope(\"output\") as output_scope:\n",
    "        \n",
    "        with tf.variable_scope(tf.get_variable_scope()) as vscope:\n",
    "#             with tf.variable_scope(\"var_scope\", reuse=True): # to reuse variable \n",
    "                W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[num_filters*len(filter_sizes), num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "    #             b = tf.get_variable(shape=[num_classes], name=\"b\", initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")#x,w,b, computes matmul(x,w)+b\n",
    "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "                tf.get_variable_scope().reuse_variables() \n",
    "                \n",
    "        # Calculate mean cross-entropy loss (with regularization)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "    \n",
    "a = Word_CNN_Model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT = True\n",
      "BATCH_SIZE = 128\n",
      "CHECKPOINT_EVERY = 100\n",
      "EMBEDDING_DIM = 128\n",
      "EVALUATE_EVERY = 100\n",
      "FILTER_SIZES = 3,4,5\n",
      "KEEP_PROB = 0.5\n",
      "L2_REG_LAMBDA = 0.0\n",
      "LOG_DEVICE_PLACEMENT = False\n",
      "NEG_LINK = https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg\n",
      "NUM_CHECKPOINTS = 5\n",
      "NUM_EPOCHS = 5\n",
      "NUM_FILTERS = 128\n",
      "NUM_FOLDS = 2\n",
      "POS_LINK = https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos\n",
      "TEST_SAMPLE_PERCENTAGE = 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_string(\"pos_link\", 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos', 'positive link')\n",
    "tf.flags.DEFINE_string(\"neg_link\", 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg', 'neg link')\n",
    "tf.flags.DEFINE_float(\"test_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_float(\"num_folds\", 2, \"Number of folds in cross-validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 5, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on testing set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for k,v in sorted(FLAGS.__flags.items()):\n",
    "    print(k.upper(), \"=\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_text, y = load_data_and_labels(FLAGS.pos_link, FLAGS.neg_link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  18759\n",
      "(10662, 266) [[    1     2     3 ...,     0     0     0]\n",
      " [    1    31    32 ...,     0     0     0]\n",
      " [   57    58    59 ...,     0     0     0]\n",
      " ..., \n",
      " [   75    84  1949 ...,     0     0     0]\n",
      " [    1  2191  2690 ...,     0     0     0]\n",
      " [11512     3   147 ...,     0     0     0]]\n",
      "[ 7359  5573 10180 ...,  1344  7293  1289]\n",
      "Train/Test split: 9595/1067\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x) for x in x_text])\n",
    "\n",
    "#convert document to [word_ids]\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#Learn the vocabulary dictionary and give matrix of indexies of words (Word-Id matrix)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text))) \n",
    "print(\"Vocabulary Size: \", len(vocab_processor.vocabulary_))\n",
    "\n",
    "print(x.shape, x)\n",
    "\n",
    "    \n",
    "#permutation of data\n",
    "np.random.seed(10)\n",
    "indices = np.random.permutation(range(len(x)))\n",
    "print(indices)\n",
    "x_shuffled = x[indices]\n",
    "y_shuffled = y[indices]\n",
    "\n",
    "# Split train/test set\n",
    "# We first test with simple split, and use cross-validation later\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled, test_size=FLAGS.test_sample_percentage, random_state=1 )\n",
    "\n",
    "\n",
    "# test_sample_index = -1 * int(FLAGS.test_sample_percentage * float(len(y)))\n",
    "# x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "# y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]\n",
    "\n",
    "# del x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "def train():\n",
    "\n",
    "      with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        \n",
    "#         sess = tf.Session(config=session_conf)\n",
    "#         with sess.as_default(): #\n",
    "        with tf.Session(config=session_conf) as sess:\n",
    "            #init model\n",
    "            cnn = Word_CNN_Model(\n",
    "                input_dim=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            ### Define training procedure\n",
    "            #global step value to be used throughout training and testing\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            #define Adam optim\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "            #compute gradients\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss) #cnn.loss is a property defined in the scope \"loss\"\n",
    "            #apply above gradients \n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step) #be used in train_step()\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    #Adding a histogram summary to visualize data's distribution in TensorBoar\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g) #(node_name, values)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries) # merge all grad summaries\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time())) \n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp)) #out_dir=./runs/timestamp\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss) # a summary to monitor cost tensor\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy) #a summary to monitor accuracy\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged]) #will be used in train_step()\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Test summaries\n",
    "            test_summary_op = tf.summary.merge([loss_summary, acc_summary])# be used in test_step()\n",
    "            test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "            test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\")) #./runs/timestamp/checkpoints\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\") #prefix of filename\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir) # default, TF assumes checkpoint dir exists\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Save vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\")) # ./runs/timestamp/vocab\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.keep_prob: FLAGS.keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict) # run all summaries with the input=feed_dict\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                #display info: 2017-12-08T16:00:45.606711: step 8, loss 3.45242, acc 0.5\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                #add summary\n",
    "                train_summary_writer.add_summary(summaries, step) #(buffer, global step value)\n",
    "\n",
    "            def test_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a test set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, test_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy)) \n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches (from preprocessing part above)\n",
    "            batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            \n",
    "            # Training loop. Note, nb of batchs/iterations = len(data)/batch_size\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step) #get the current step of training\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    \n",
    "    \n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
