{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/dennybritz/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from urllib.request import urlopen\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_data_and_labels(pos_link, neg_link):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "#     # Pull sentences with positive sentiment\n",
    "    pos_file = urlopen(pos_link)\n",
    " \n",
    "    # Pull sentences with negative sentiment\n",
    "    neg_file = urlopen(neg_link)\n",
    "\n",
    "    # Load data from files\n",
    "    positive_examples = list(pos_file.readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(neg_file.readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    \n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent.decode('latin1')) for sent in x_text] \n",
    "    # or:  x_text = [clean_str(str(sent)) for sent in x_text]\n",
    "\n",
    "    \n",
    "    # Generate labels (with two types)\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    \n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1 #nb of iterations per epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index] #return a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (?, 1, 1, 128) (?, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Word_CNN_Model:\n",
    "    \"\"\"Word-level CNN model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=32, num_classes=2, vocab_size=59, embedding_size=128, \n",
    "            filter_sizes=[7], num_filters=128, l2_reg_lambda=0.0, dtype=tf.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "            Inputs:\n",
    "            - input_dim: Tuple (H, W, C) giving size of input data\n",
    "            - num_classes: Number of scores to produce from the final affine layer\n",
    "            - vocab_size: Number of vocabs\n",
    "            - embedding_size: size of embedded text\n",
    "            - num_filters: Number of filters to use in the convolutional layer\n",
    "            - filter_size: Size of filters to use in the convolutional layer\n",
    "            - weight_scale: Scalar giving standard deviation for random initialization\n",
    "              of weights.\n",
    "            - l2_reg_lambda: Scalar giving L2 regularization strength\n",
    "            - dtype: tf datatype to use for computation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_x = tf.placeholder(dtype=tf.int32, shape=[None, input_dim], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='dropout')\n",
    "        \n",
    "        l2_loss = tf.constant(0.0) #l2 regularization loss (optional)\n",
    " \n",
    "        #Embedding layer\n",
    "        with tf.device(\"/cpu:0\"), tf.name_scope(\"embedding\"):\n",
    "            gen = tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)\n",
    "            self.W = tf.Variable (gen, name =\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1) #expand one more dimension\n",
    "            \n",
    "        # Create a set of blocks\n",
    "        pooled_outputs = []\n",
    "        for i, filtersize in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv_maxpool_block_%s\" %filtersize) as main_scope: \n",
    "\n",
    "                    ###conv layer\n",
    "    #                 #conv+relu\n",
    "    #                 conv = tf.layers.conv2d(inputs=self.embedded_chars_expanded, filters=num_filters, \n",
    "    #                                         kernel_size=(filtersize, embedding_size), \n",
    "    # #                                         kernel_initializer= tf.truncated_normal(shape=[filtersize, embedding_size], stddev=0.1),\n",
    "    #                                         strides=(1,1), padding='valid', activation='relu',\n",
    "    # #                                         bias_initializer=tf.constant(0.1, shape=[num_filters]), \n",
    "    #                                         name=str(\"conv_%d\"%i) )\n",
    "    #                 #pool\n",
    "    #                 pooled = tf.layers.max_pooling2d(conv, pool_size=(1, sequence_length - filter_size + 1),\n",
    "    #                                                  strides=(1,1), padding='valid', name=str('pool_%d'%i))\n",
    "\n",
    "                    #define parameters\n",
    "                    ran = tf.truncated_normal(shape=[filtersize, embedding_size, 1, num_filters], stddev=0.1)\n",
    "                    W = tf.Variable(ran, name='W')\n",
    "#                     W = tf.get_variable(\"W\", initializer=ran)\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b_%d\" %i)\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        input=self.embedded_chars_expanded,\n",
    "                        filter=W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=str(\"conv%d\"%i))\n",
    "                    # relu\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=str(\"relu%d\"%i))\n",
    "                    # maxpool\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, input_dim - filtersize + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=str(\"pool%d\"%i))\n",
    "\n",
    "                    pooled_outputs.append(pooled)\n",
    "\n",
    "        # combine all pools\n",
    "        h_pool = tf.concat(pooled_outputs, 3) #Concatenates tensors along  dimension 3\n",
    "        self.h_pool = tf.reshape(h_pool, [-1, num_filters*len(filter_sizes)]) #flatten\n",
    "        print(len(pooled_outputs), h_pool.get_shape(), self.h_pool.get_shape()) #1 (?, 1, 1, 128) (?, 128)\n",
    "        \n",
    "        #dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool, keep_prob=self.keep_prob)\n",
    "            \n",
    "        # Final (unnormalized) scores and predictions\n",
    "#         with tf.name_scope(\"output\") as output_scope:\n",
    "        \n",
    "        with tf.variable_scope(tf.get_variable_scope()) as vscope:\n",
    "#             with tf.variable_scope(\"var_scope\", reuse=True): # to reuse variable \n",
    "                W = tf.get_variable(\n",
    "                    \"W\",\n",
    "                    shape=[num_filters*len(filter_sizes), num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "    #             b = tf.get_variable(shape=[num_classes], name=\"b\", initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")#x,w,b, computes matmul(x,w)+b\n",
    "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "                tf.get_variable_scope().reuse_variables() \n",
    "                \n",
    "        # Calculate mean cross-entropy loss (with regularization)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "    \n",
    "a = Word_CNN_Model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT = True\n",
      "BATCH_SIZE = 128\n",
      "CHECKPOINT_EVERY = 100\n",
      "EMBEDDING_DIM = 128\n",
      "EVALUATE_EVERY = 100\n",
      "FILTER_SIZES = 3,4,5\n",
      "KEEP_PROB = 0.5\n",
      "L2_REG_LAMBDA = 0.0\n",
      "LOG_DEVICE_PLACEMENT = False\n",
      "NEG_LINK = https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg\n",
      "NUM_CHECKPOINTS = 5\n",
      "NUM_EPOCHS = 5\n",
      "NUM_FILTERS = 128\n",
      "NUM_FOLDS = 2\n",
      "POS_LINK = https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos\n",
      "TEST_SAMPLE_PERCENTAGE = 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_string(\"pos_link\", 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos', 'positive link')\n",
    "tf.flags.DEFINE_string(\"neg_link\", 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg', 'neg link')\n",
    "tf.flags.DEFINE_float(\"test_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_float(\"num_folds\", 2, \"Number of folds in cross-validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 5, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on testing set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for k,v in sorted(FLAGS.__flags.items()):\n",
    "    print(k.upper(), \"=\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_text, y = load_data_and_labels(FLAGS.pos_link, FLAGS.neg_link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  18759\n",
      "(10662, 266) [[    1     2     3 ...,     0     0     0]\n",
      " [    1    31    32 ...,     0     0     0]\n",
      " [   57    58    59 ...,     0     0     0]\n",
      " ..., \n",
      " [   75    84  1949 ...,     0     0     0]\n",
      " [    1  2191  2690 ...,     0     0     0]\n",
      " [11512     3   147 ...,     0     0     0]]\n",
      "[ 7359  5573 10180 ...,  1344  7293  1289]\n",
      "Train/Test split: 9595/1067\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x) for x in x_text])\n",
    "\n",
    "#convert document to [word_ids]\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#Learn the vocabulary dictionary and give matrix of indexies of words (Word-Id matrix)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text))) \n",
    "print(\"Vocabulary Size: \", len(vocab_processor.vocabulary_))\n",
    "\n",
    "print(x.shape, x)\n",
    "\n",
    "    \n",
    "#permutation of data\n",
    "np.random.seed(10)\n",
    "indices = np.random.permutation(range(len(x)))\n",
    "print(indices)\n",
    "x_shuffled = x[indices]\n",
    "y_shuffled = y[indices]\n",
    "\n",
    "# Split train/test set\n",
    "# We first test with simple split, and use cross-validation later\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled, test_size=FLAGS.test_sample_percentage, random_state=1 )\n",
    "\n",
    "\n",
    "# test_sample_index = -1 * int(FLAGS.test_sample_percentage * float(len(y)))\n",
    "# x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "# y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]\n",
    "\n",
    "# del x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (?, 1, 1, 384) (?, 384)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/W:0/grad/hist is illegal; using conv_maxpool_block_3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/W:0/grad/sparsity is illegal; using conv_maxpool_block_3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/b_0:0/grad/hist is illegal; using conv_maxpool_block_3/b_0_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/b_0:0/grad/sparsity is illegal; using conv_maxpool_block_3/b_0_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/W:0/grad/hist is illegal; using conv_maxpool_block_4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/W:0/grad/sparsity is illegal; using conv_maxpool_block_4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/b_1:0/grad/hist is illegal; using conv_maxpool_block_4/b_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/b_1:0/grad/sparsity is illegal; using conv_maxpool_block_4/b_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/W:0/grad/hist is illegal; using conv_maxpool_block_5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/W:0/grad/sparsity is illegal; using conv_maxpool_block_5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/b_2:0/grad/hist is illegal; using conv_maxpool_block_5/b_2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/b_2:0/grad/sparsity is illegal; using conv_maxpool_block_5/b_2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name b:0/grad/hist is illegal; using b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name b:0/grad/sparsity is illegal; using b_0/grad/sparsity instead.\n",
      "Writing to /home/hoang/Documents/svn/reposn/code/text-classification/runs/1513244435\n",
      "\n",
      "2017-12-14T10:40:37.947859: step 1, loss 2.21926, acc 0.5625\n",
      "2017-12-14T10:40:39.602479: step 2, loss 1.93364, acc 0.492188\n",
      "2017-12-14T10:40:41.005219: step 3, loss 1.80823, acc 0.539062\n",
      "2017-12-14T10:40:42.516095: step 4, loss 2.57066, acc 0.492188\n",
      "2017-12-14T10:40:43.930835: step 5, loss 2.15045, acc 0.460938\n",
      "2017-12-14T10:40:45.492492: step 6, loss 1.91225, acc 0.539062\n",
      "2017-12-14T10:40:47.092426: step 7, loss 1.50022, acc 0.546875\n",
      "2017-12-14T10:40:48.533027: step 8, loss 1.85176, acc 0.5\n",
      "2017-12-14T10:40:49.913260: step 9, loss 1.90315, acc 0.546875\n",
      "2017-12-14T10:40:51.228916: step 10, loss 2.19611, acc 0.484375\n",
      "2017-12-14T10:40:52.602693: step 11, loss 2.31246, acc 0.5625\n",
      "2017-12-14T10:40:53.935482: step 12, loss 1.97392, acc 0.546875\n",
      "2017-12-14T10:40:55.372921: step 13, loss 1.53357, acc 0.578125\n",
      "2017-12-14T10:40:56.952283: step 14, loss 1.67178, acc 0.539062\n",
      "2017-12-14T10:40:58.434834: step 15, loss 1.86448, acc 0.492188\n",
      "2017-12-14T10:40:59.823185: step 16, loss 1.44468, acc 0.59375\n",
      "2017-12-14T10:41:01.164473: step 17, loss 1.7112, acc 0.578125\n",
      "2017-12-14T10:41:02.489795: step 18, loss 1.96577, acc 0.523438\n",
      "2017-12-14T10:41:03.831947: step 19, loss 2.06417, acc 0.523438\n",
      "2017-12-14T10:41:05.348725: step 20, loss 2.0125, acc 0.539062\n",
      "2017-12-14T10:41:06.915507: step 21, loss 1.74418, acc 0.492188\n",
      "2017-12-14T10:41:08.338597: step 22, loss 2.05169, acc 0.53125\n",
      "2017-12-14T10:41:09.853335: step 23, loss 2.23457, acc 0.507812\n",
      "2017-12-14T10:41:11.214207: step 24, loss 2.17749, acc 0.460938\n",
      "2017-12-14T10:41:12.579751: step 25, loss 1.67702, acc 0.5625\n",
      "2017-12-14T10:41:13.940151: step 26, loss 1.9276, acc 0.523438\n",
      "2017-12-14T10:41:15.470409: step 27, loss 1.11492, acc 0.632812\n",
      "2017-12-14T10:41:17.023928: step 28, loss 2.19423, acc 0.460938\n",
      "2017-12-14T10:41:18.447630: step 29, loss 2.15472, acc 0.53125\n",
      "2017-12-14T10:41:19.801613: step 30, loss 1.92268, acc 0.507812\n",
      "2017-12-14T10:41:21.165029: step 31, loss 1.98649, acc 0.515625\n",
      "2017-12-14T10:41:22.484267: step 32, loss 1.77944, acc 0.554688\n",
      "2017-12-14T10:41:23.963284: step 33, loss 1.87719, acc 0.523438\n",
      "2017-12-14T10:41:25.499788: step 34, loss 1.91077, acc 0.507812\n",
      "2017-12-14T10:41:27.211263: step 35, loss 1.80994, acc 0.523438\n",
      "2017-12-14T10:41:28.676324: step 36, loss 1.71828, acc 0.53125\n",
      "2017-12-14T10:41:30.036532: step 37, loss 1.56908, acc 0.507812\n",
      "2017-12-14T10:41:31.395693: step 38, loss 1.92623, acc 0.523438\n",
      "2017-12-14T10:41:32.715125: step 39, loss 1.5828, acc 0.523438\n",
      "2017-12-14T10:41:34.116962: step 40, loss 1.4387, acc 0.578125\n",
      "2017-12-14T10:41:35.736716: step 41, loss 1.45141, acc 0.53125\n",
      "2017-12-14T10:41:37.232463: step 42, loss 1.51083, acc 0.546875\n",
      "2017-12-14T10:41:38.651682: step 43, loss 1.69975, acc 0.5\n",
      "2017-12-14T10:41:40.169958: step 44, loss 1.72947, acc 0.484375\n",
      "2017-12-14T10:41:41.681645: step 45, loss 1.74782, acc 0.476562\n",
      "2017-12-14T10:41:43.168873: step 46, loss 1.37485, acc 0.570312\n",
      "2017-12-14T10:41:44.683087: step 47, loss 1.39621, acc 0.59375\n",
      "2017-12-14T10:41:46.507915: step 48, loss 1.36613, acc 0.609375\n",
      "2017-12-14T10:41:48.096708: step 49, loss 1.59879, acc 0.5625\n",
      "2017-12-14T10:41:49.667993: step 50, loss 1.48147, acc 0.507812\n",
      "2017-12-14T10:41:51.095347: step 51, loss 1.2781, acc 0.5625\n",
      "2017-12-14T10:41:52.522514: step 52, loss 1.49234, acc 0.5\n",
      "2017-12-14T10:41:53.962792: step 53, loss 1.21444, acc 0.617188\n",
      "2017-12-14T10:41:55.840724: step 54, loss 1.54912, acc 0.515625\n",
      "2017-12-14T10:41:58.038748: step 55, loss 1.38576, acc 0.578125\n",
      "2017-12-14T10:41:59.507669: step 56, loss 1.69001, acc 0.492188\n",
      "2017-12-14T10:42:00.941488: step 57, loss 1.76723, acc 0.5\n",
      "2017-12-14T10:42:02.460472: step 58, loss 1.80356, acc 0.476562\n",
      "2017-12-14T10:42:03.978537: step 59, loss 1.49344, acc 0.539062\n",
      "2017-12-14T10:42:05.713580: step 60, loss 1.44559, acc 0.5625\n",
      "2017-12-14T10:42:07.421175: step 61, loss 1.59424, acc 0.546875\n",
      "2017-12-14T10:42:09.192933: step 62, loss 1.44712, acc 0.53125\n",
      "2017-12-14T10:42:10.717883: step 63, loss 1.58343, acc 0.554688\n",
      "2017-12-14T10:42:12.176542: step 64, loss 1.14773, acc 0.671875\n",
      "2017-12-14T10:42:13.607047: step 65, loss 1.03109, acc 0.625\n",
      "2017-12-14T10:42:15.147467: step 66, loss 1.33819, acc 0.578125\n",
      "2017-12-14T10:42:16.623758: step 67, loss 1.42889, acc 0.5\n",
      "2017-12-14T10:42:18.273849: step 68, loss 1.57457, acc 0.539062\n",
      "2017-12-14T10:42:19.775270: step 69, loss 1.99803, acc 0.484375\n",
      "2017-12-14T10:42:21.252696: step 70, loss 1.55505, acc 0.5\n",
      "2017-12-14T10:42:22.640996: step 71, loss 1.48287, acc 0.53125\n",
      "2017-12-14T10:42:24.142096: step 72, loss 1.45766, acc 0.554688\n",
      "2017-12-14T10:42:25.763742: step 73, loss 1.56157, acc 0.523438\n",
      "2017-12-14T10:42:27.371356: step 74, loss 1.62375, acc 0.46875\n",
      "2017-12-14T10:42:28.741116: step 75, loss 1.36347, acc 0.569106\n",
      "2017-12-14T10:42:30.219770: step 76, loss 1.35214, acc 0.570312\n",
      "2017-12-14T10:42:31.655946: step 77, loss 1.10087, acc 0.578125\n",
      "2017-12-14T10:42:33.009537: step 78, loss 1.58596, acc 0.484375\n",
      "2017-12-14T10:42:34.432924: step 79, loss 1.41554, acc 0.539062\n",
      "2017-12-14T10:42:36.065745: step 80, loss 1.4551, acc 0.539062\n",
      "2017-12-14T10:42:37.650058: step 81, loss 1.39212, acc 0.554688\n",
      "2017-12-14T10:42:39.099623: step 82, loss 1.21586, acc 0.585938\n",
      "2017-12-14T10:42:40.494921: step 83, loss 1.44497, acc 0.546875\n",
      "2017-12-14T10:42:41.904609: step 84, loss 1.31386, acc 0.585938\n",
      "2017-12-14T10:42:43.298851: step 85, loss 1.63946, acc 0.515625\n",
      "2017-12-14T10:42:44.718383: step 86, loss 1.22316, acc 0.539062\n",
      "2017-12-14T10:42:46.346048: step 87, loss 1.16105, acc 0.59375\n",
      "2017-12-14T10:42:47.969798: step 88, loss 1.36147, acc 0.617188\n",
      "2017-12-14T10:42:49.359938: step 89, loss 1.21867, acc 0.585938\n",
      "2017-12-14T10:42:50.715915: step 90, loss 1.08855, acc 0.585938\n",
      "2017-12-14T10:42:52.116897: step 91, loss 1.08293, acc 0.648438\n",
      "2017-12-14T10:42:53.528593: step 92, loss 1.3292, acc 0.578125\n",
      "2017-12-14T10:42:54.998654: step 93, loss 0.902195, acc 0.65625\n",
      "2017-12-14T10:42:56.526532: step 94, loss 1.00807, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-14T10:42:58.068132: step 95, loss 1.27776, acc 0.492188\n",
      "2017-12-14T10:42:59.409642: step 96, loss 1.40586, acc 0.460938\n",
      "2017-12-14T10:43:00.762664: step 97, loss 1.06197, acc 0.625\n",
      "2017-12-14T10:43:02.213684: step 98, loss 1.1804, acc 0.601562\n",
      "2017-12-14T10:43:03.598473: step 99, loss 0.981073, acc 0.632812\n",
      "2017-12-14T10:43:05.223072: step 100, loss 1.13196, acc 0.570312\n",
      "\n",
      "Evaluation:\n",
      "2017-12-14T10:43:09.072536: step 100, loss 0.700377, acc 0.609185\n",
      "\n",
      "Saved model checkpoint to /home/hoang/Documents/svn/reposn/code/text-classification/runs/1513244435/checkpoints/model-100\n",
      "\n",
      "2017-12-14T10:43:10.667661: step 101, loss 1.30117, acc 0.554688\n",
      "2017-12-14T10:43:12.045126: step 102, loss 1.23259, acc 0.625\n",
      "2017-12-14T10:43:13.577521: step 103, loss 1.2361, acc 0.554688\n",
      "2017-12-14T10:43:15.066985: step 104, loss 1.12002, acc 0.617188\n",
      "2017-12-14T10:43:16.564321: step 105, loss 1.02452, acc 0.65625\n",
      "2017-12-14T10:43:18.341632: step 106, loss 1.02739, acc 0.65625\n",
      "2017-12-14T10:43:19.813225: step 107, loss 1.01199, acc 0.585938\n",
      "2017-12-14T10:43:21.265459: step 108, loss 1.0907, acc 0.585938\n",
      "2017-12-14T10:43:22.738054: step 109, loss 1.02051, acc 0.601562\n",
      "2017-12-14T10:43:24.443920: step 110, loss 1.58128, acc 0.453125\n",
      "2017-12-14T10:43:25.916087: step 111, loss 1.29944, acc 0.570312\n",
      "2017-12-14T10:43:27.422674: step 112, loss 1.02669, acc 0.640625\n",
      "2017-12-14T10:43:29.020922: step 113, loss 1.1072, acc 0.570312\n",
      "2017-12-14T10:43:30.382218: step 114, loss 1.38923, acc 0.554688\n",
      "2017-12-14T10:43:31.894621: step 115, loss 1.04527, acc 0.578125\n",
      "2017-12-14T10:43:33.474318: step 116, loss 1.15368, acc 0.585938\n",
      "2017-12-14T10:43:34.989069: step 117, loss 0.982601, acc 0.578125\n",
      "2017-12-14T10:43:36.530854: step 118, loss 1.03711, acc 0.570312\n",
      "2017-12-14T10:43:38.001049: step 119, loss 1.12836, acc 0.601562\n",
      "2017-12-14T10:43:39.566179: step 120, loss 1.04583, acc 0.609375\n",
      "2017-12-14T10:43:41.293547: step 121, loss 1.09241, acc 0.632812\n",
      "2017-12-14T10:43:42.905032: step 122, loss 1.22551, acc 0.578125\n",
      "2017-12-14T10:43:44.354043: step 123, loss 1.03831, acc 0.625\n",
      "2017-12-14T10:43:45.846903: step 124, loss 1.19799, acc 0.515625\n",
      "2017-12-14T10:43:47.344791: step 125, loss 1.11735, acc 0.578125\n",
      "2017-12-14T10:43:48.874891: step 126, loss 0.997982, acc 0.578125\n",
      "2017-12-14T10:43:50.368770: step 127, loss 0.996121, acc 0.59375\n",
      "2017-12-14T10:43:51.816285: step 128, loss 1.31759, acc 0.515625\n",
      "2017-12-14T10:43:53.303395: step 129, loss 1.07354, acc 0.59375\n",
      "2017-12-14T10:43:54.732406: step 130, loss 1.21172, acc 0.523438\n",
      "2017-12-14T10:43:56.299919: step 131, loss 1.08849, acc 0.578125\n",
      "2017-12-14T10:43:57.936021: step 132, loss 0.950299, acc 0.5625\n",
      "2017-12-14T10:43:59.316753: step 133, loss 0.916035, acc 0.570312\n",
      "2017-12-14T10:44:00.661791: step 134, loss 0.969995, acc 0.625\n",
      "2017-12-14T10:44:02.170939: step 135, loss 0.913175, acc 0.609375\n",
      "2017-12-14T10:44:03.552884: step 136, loss 1.25581, acc 0.578125\n",
      "2017-12-14T10:44:05.050744: step 137, loss 0.912388, acc 0.609375\n",
      "2017-12-14T10:44:06.591812: step 138, loss 0.909802, acc 0.585938\n",
      "2017-12-14T10:44:08.197316: step 139, loss 1.03133, acc 0.59375\n",
      "2017-12-14T10:44:09.679633: step 140, loss 0.989476, acc 0.554688\n",
      "2017-12-14T10:44:11.213159: step 141, loss 1.046, acc 0.640625\n",
      "2017-12-14T10:44:12.593815: step 142, loss 1.03115, acc 0.578125\n",
      "2017-12-14T10:44:14.001424: step 143, loss 0.995224, acc 0.617188\n",
      "2017-12-14T10:44:15.394261: step 144, loss 1.07981, acc 0.585938\n",
      "2017-12-14T10:44:17.021309: step 145, loss 1.20976, acc 0.53125\n",
      "2017-12-14T10:44:18.889311: step 146, loss 0.86452, acc 0.65625\n",
      "2017-12-14T10:44:20.655437: step 147, loss 1.25639, acc 0.546875\n",
      "2017-12-14T10:44:22.142343: step 148, loss 0.974645, acc 0.617188\n",
      "2017-12-14T10:44:23.547803: step 149, loss 0.869914, acc 0.609375\n",
      "2017-12-14T10:44:25.188949: step 150, loss 0.786286, acc 0.682927\n",
      "2017-12-14T10:44:26.789259: step 151, loss 0.884652, acc 0.640625\n",
      "2017-12-14T10:44:28.351006: step 152, loss 0.748124, acc 0.648438\n",
      "2017-12-14T10:44:29.771732: step 153, loss 0.71071, acc 0.703125\n",
      "2017-12-14T10:44:31.221377: step 154, loss 0.78204, acc 0.664062\n",
      "2017-12-14T10:44:32.654943: step 155, loss 0.892431, acc 0.65625\n",
      "2017-12-14T10:44:34.240992: step 156, loss 1.01436, acc 0.585938\n",
      "2017-12-14T10:44:35.863299: step 157, loss 1.02947, acc 0.570312\n",
      "2017-12-14T10:44:37.465100: step 158, loss 0.877742, acc 0.640625\n",
      "2017-12-14T10:44:39.090480: step 159, loss 1.12402, acc 0.546875\n",
      "2017-12-14T10:44:40.499065: step 160, loss 0.80545, acc 0.671875\n",
      "2017-12-14T10:44:41.919406: step 161, loss 0.874099, acc 0.640625\n",
      "2017-12-14T10:44:43.280157: step 162, loss 0.907763, acc 0.640625\n",
      "2017-12-14T10:44:44.703121: step 163, loss 0.771133, acc 0.671875\n",
      "2017-12-14T10:44:46.370155: step 164, loss 0.777782, acc 0.671875\n",
      "2017-12-14T10:44:47.958967: step 165, loss 0.871456, acc 0.585938\n",
      "2017-12-14T10:44:49.403601: step 166, loss 0.758879, acc 0.65625\n",
      "2017-12-14T10:44:50.859825: step 167, loss 0.851027, acc 0.6875\n",
      "2017-12-14T10:44:52.251448: step 168, loss 0.67463, acc 0.679688\n",
      "2017-12-14T10:44:53.614984: step 169, loss 0.813435, acc 0.65625\n",
      "2017-12-14T10:44:55.076133: step 170, loss 0.718737, acc 0.742188\n",
      "2017-12-14T10:44:56.493820: step 171, loss 0.882015, acc 0.609375\n",
      "2017-12-14T10:44:58.227566: step 172, loss 0.785867, acc 0.695312\n",
      "2017-12-14T10:44:59.760111: step 173, loss 0.734468, acc 0.617188\n",
      "2017-12-14T10:45:01.133234: step 174, loss 0.788676, acc 0.671875\n",
      "2017-12-14T10:45:02.508625: step 175, loss 0.866585, acc 0.648438\n",
      "2017-12-14T10:45:03.907991: step 176, loss 0.848776, acc 0.65625\n",
      "2017-12-14T10:45:05.395023: step 177, loss 0.833033, acc 0.65625\n",
      "2017-12-14T10:45:07.004478: step 178, loss 0.889549, acc 0.625\n",
      "2017-12-14T10:45:08.470636: step 179, loss 0.953575, acc 0.609375\n",
      "2017-12-14T10:45:09.937441: step 180, loss 0.97243, acc 0.578125\n",
      "2017-12-14T10:45:11.304401: step 181, loss 0.777577, acc 0.59375\n",
      "2017-12-14T10:45:12.662104: step 182, loss 0.800546, acc 0.648438\n",
      "2017-12-14T10:45:14.131080: step 183, loss 0.751824, acc 0.640625\n",
      "2017-12-14T10:45:15.631834: step 184, loss 0.943532, acc 0.632812\n",
      "2017-12-14T10:45:17.171183: step 185, loss 0.932589, acc 0.609375\n",
      "2017-12-14T10:45:18.739904: step 186, loss 0.832069, acc 0.578125\n",
      "2017-12-14T10:45:20.224273: step 187, loss 0.863369, acc 0.609375\n",
      "2017-12-14T10:45:21.607719: step 188, loss 0.698987, acc 0.6875\n",
      "2017-12-14T10:45:23.024153: step 189, loss 0.657246, acc 0.703125\n",
      "2017-12-14T10:45:24.407921: step 190, loss 0.811582, acc 0.664062\n",
      "2017-12-14T10:45:26.079387: step 191, loss 0.925757, acc 0.625\n",
      "2017-12-14T10:45:27.588580: step 192, loss 0.793386, acc 0.640625\n",
      "2017-12-14T10:45:29.121160: step 193, loss 0.811166, acc 0.648438\n",
      "2017-12-14T10:45:30.497386: step 194, loss 0.901964, acc 0.695312\n",
      "2017-12-14T10:45:31.993677: step 195, loss 0.865847, acc 0.625\n",
      "2017-12-14T10:45:33.459627: step 196, loss 0.829582, acc 0.648438\n",
      "2017-12-14T10:45:34.929895: step 197, loss 0.828322, acc 0.671875\n",
      "2017-12-14T10:45:36.399628: step 198, loss 0.612804, acc 0.71875\n",
      "2017-12-14T10:45:38.046803: step 199, loss 0.786741, acc 0.65625\n",
      "2017-12-14T10:45:39.571420: step 200, loss 0.881468, acc 0.601562\n",
      "\n",
      "Evaluation:\n",
      "2017-12-14T10:45:43.080749: step 200, loss 0.609088, acc 0.667291\n",
      "\n",
      "Saved model checkpoint to /home/hoang/Documents/svn/reposn/code/text-classification/runs/1513244435/checkpoints/model-200\n",
      "\n",
      "2017-12-14T10:45:44.597243: step 201, loss 0.822716, acc 0.617188\n",
      "2017-12-14T10:45:46.212212: step 202, loss 0.727063, acc 0.648438\n",
      "2017-12-14T10:45:47.681225: step 203, loss 0.929917, acc 0.554688\n",
      "2017-12-14T10:45:49.074370: step 204, loss 0.693632, acc 0.671875\n",
      "2017-12-14T10:45:50.473209: step 205, loss 0.611201, acc 0.710938\n",
      "2017-12-14T10:45:51.828158: step 206, loss 0.804455, acc 0.625\n",
      "2017-12-14T10:45:53.162030: step 207, loss 0.878129, acc 0.632812\n",
      "2017-12-14T10:45:54.545058: step 208, loss 0.872862, acc 0.640625\n",
      "2017-12-14T10:45:56.072939: step 209, loss 0.768905, acc 0.664062\n",
      "2017-12-14T10:45:57.584692: step 210, loss 0.804681, acc 0.59375\n",
      "2017-12-14T10:45:59.143114: step 211, loss 0.719549, acc 0.632812\n",
      "2017-12-14T10:46:00.506736: step 212, loss 0.815127, acc 0.601562\n",
      "2017-12-14T10:46:01.903422: step 213, loss 0.741475, acc 0.679688\n",
      "2017-12-14T10:46:03.297944: step 214, loss 0.815426, acc 0.632812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-14T10:46:04.778173: step 215, loss 0.880674, acc 0.585938\n",
      "2017-12-14T10:46:06.351376: step 216, loss 0.949091, acc 0.523438\n",
      "2017-12-14T10:46:07.821899: step 217, loss 0.67371, acc 0.6875\n",
      "2017-12-14T10:46:09.343492: step 218, loss 0.815117, acc 0.640625\n",
      "2017-12-14T10:46:10.756151: step 219, loss 0.763286, acc 0.648438\n",
      "2017-12-14T10:46:12.119648: step 220, loss 0.780008, acc 0.617188\n",
      "2017-12-14T10:46:13.473375: step 221, loss 0.643472, acc 0.703125\n",
      "2017-12-14T10:46:14.973780: step 222, loss 0.703708, acc 0.648438\n",
      "2017-12-14T10:46:16.445631: step 223, loss 0.744776, acc 0.640625\n",
      "2017-12-14T10:46:17.892007: step 224, loss 0.735036, acc 0.625\n",
      "2017-12-14T10:46:19.311487: step 225, loss 0.887355, acc 0.626016\n",
      "2017-12-14T10:46:20.659822: step 226, loss 0.86387, acc 0.601562\n",
      "2017-12-14T10:46:22.226788: step 227, loss 0.605147, acc 0.710938\n",
      "2017-12-14T10:46:23.628554: step 228, loss 0.683583, acc 0.65625\n",
      "2017-12-14T10:46:25.260815: step 229, loss 0.742237, acc 0.703125\n",
      "2017-12-14T10:46:26.800065: step 230, loss 0.639549, acc 0.695312\n",
      "2017-12-14T10:46:28.320383: step 231, loss 0.721884, acc 0.65625\n",
      "2017-12-14T10:46:29.686861: step 232, loss 0.650668, acc 0.695312\n",
      "2017-12-14T10:46:31.150431: step 233, loss 0.825764, acc 0.625\n",
      "2017-12-14T10:46:32.511185: step 234, loss 0.764391, acc 0.578125\n",
      "2017-12-14T10:46:34.031581: step 235, loss 0.56777, acc 0.703125\n",
      "2017-12-14T10:46:35.572960: step 236, loss 0.751025, acc 0.695312\n",
      "2017-12-14T10:46:37.199596: step 237, loss 0.643053, acc 0.65625\n",
      "2017-12-14T10:46:38.633238: step 238, loss 0.713273, acc 0.664062\n",
      "2017-12-14T10:46:40.009905: step 239, loss 0.79229, acc 0.554688\n",
      "2017-12-14T10:46:41.410018: step 240, loss 0.702937, acc 0.632812\n",
      "2017-12-14T10:46:42.773571: step 241, loss 0.704254, acc 0.664062\n",
      "2017-12-14T10:46:44.199204: step 242, loss 0.59379, acc 0.664062\n",
      "2017-12-14T10:46:45.780656: step 243, loss 0.629723, acc 0.71875\n",
      "2017-12-14T10:46:47.307187: step 244, loss 0.795813, acc 0.609375\n",
      "2017-12-14T10:46:48.715132: step 245, loss 0.765543, acc 0.609375\n",
      "2017-12-14T10:46:50.134837: step 246, loss 0.590029, acc 0.703125\n",
      "2017-12-14T10:46:51.545412: step 247, loss 0.609815, acc 0.703125\n",
      "2017-12-14T10:46:53.136207: step 248, loss 0.826826, acc 0.578125\n",
      "2017-12-14T10:46:54.588645: step 249, loss 0.782027, acc 0.671875\n",
      "2017-12-14T10:46:56.190899: step 250, loss 0.605908, acc 0.695312\n",
      "2017-12-14T10:46:57.803448: step 251, loss 0.626018, acc 0.695312\n",
      "2017-12-14T10:46:59.162860: step 252, loss 0.547651, acc 0.71875\n",
      "2017-12-14T10:47:00.553553: step 253, loss 0.586175, acc 0.742188\n",
      "2017-12-14T10:47:02.017447: step 254, loss 0.615283, acc 0.679688\n",
      "2017-12-14T10:47:03.379938: step 255, loss 0.584631, acc 0.695312\n",
      "2017-12-14T10:47:04.829114: step 256, loss 0.598221, acc 0.695312\n",
      "2017-12-14T10:47:06.493042: step 257, loss 0.693159, acc 0.664062\n",
      "2017-12-14T10:47:08.081375: step 258, loss 0.649401, acc 0.65625\n",
      "2017-12-14T10:47:09.506825: step 259, loss 0.592208, acc 0.695312\n",
      "2017-12-14T10:47:10.913011: step 260, loss 0.811393, acc 0.617188\n",
      "2017-12-14T10:47:12.298243: step 261, loss 0.564349, acc 0.71875\n",
      "2017-12-14T10:47:13.764739: step 262, loss 0.731298, acc 0.632812\n",
      "2017-12-14T10:47:15.306605: step 263, loss 0.689038, acc 0.6875\n",
      "2017-12-14T10:47:16.838003: step 264, loss 0.644806, acc 0.71875\n",
      "2017-12-14T10:47:18.328034: step 265, loss 0.616077, acc 0.710938\n",
      "2017-12-14T10:47:19.747715: step 266, loss 0.783562, acc 0.570312\n",
      "2017-12-14T10:47:21.153103: step 267, loss 0.73953, acc 0.648438\n",
      "2017-12-14T10:47:22.491048: step 268, loss 0.680583, acc 0.648438\n",
      "2017-12-14T10:47:23.969897: step 269, loss 0.652067, acc 0.695312\n",
      "2017-12-14T10:47:25.439933: step 270, loss 0.726753, acc 0.679688\n",
      "2017-12-14T10:47:26.909660: step 271, loss 0.698494, acc 0.65625\n",
      "2017-12-14T10:47:28.396321: step 272, loss 0.764417, acc 0.601562\n",
      "2017-12-14T10:47:29.903129: step 273, loss 0.613866, acc 0.679688\n",
      "2017-12-14T10:47:31.413355: step 274, loss 0.583809, acc 0.6875\n",
      "2017-12-14T10:47:32.758025: step 275, loss 0.597108, acc 0.726562\n",
      "2017-12-14T10:47:34.123592: step 276, loss 0.599733, acc 0.671875\n",
      "2017-12-14T10:47:35.597354: step 277, loss 0.664568, acc 0.6875\n",
      "2017-12-14T10:47:37.086873: step 278, loss 0.675428, acc 0.65625\n",
      "2017-12-14T10:47:38.455325: step 279, loss 0.685109, acc 0.625\n",
      "2017-12-14T10:47:39.847450: step 280, loss 0.726929, acc 0.648438\n",
      "2017-12-14T10:47:41.224175: step 281, loss 0.556763, acc 0.671875\n",
      "2017-12-14T10:47:42.581713: step 282, loss 0.619169, acc 0.71875\n",
      "2017-12-14T10:47:43.959600: step 283, loss 0.779683, acc 0.59375\n",
      "2017-12-14T10:47:45.504535: step 284, loss 0.643376, acc 0.695312\n",
      "2017-12-14T10:47:47.184698: step 285, loss 0.643497, acc 0.695312\n",
      "2017-12-14T10:47:48.579354: step 286, loss 0.631435, acc 0.703125\n",
      "2017-12-14T10:47:49.959682: step 287, loss 0.645304, acc 0.664062\n",
      "2017-12-14T10:47:51.368787: step 288, loss 0.692112, acc 0.65625\n",
      "2017-12-14T10:47:52.789672: step 289, loss 0.616899, acc 0.664062\n",
      "2017-12-14T10:47:54.146569: step 290, loss 0.703108, acc 0.609375\n",
      "2017-12-14T10:47:55.708752: step 291, loss 0.592605, acc 0.710938\n",
      "2017-12-14T10:47:57.242028: step 292, loss 0.62969, acc 0.710938\n",
      "2017-12-14T10:47:58.658451: step 293, loss 0.618467, acc 0.664062\n",
      "2017-12-14T10:48:00.015147: step 294, loss 0.667159, acc 0.6875\n",
      "2017-12-14T10:48:01.399601: step 295, loss 0.671735, acc 0.726562\n",
      "2017-12-14T10:48:02.839252: step 296, loss 0.582803, acc 0.679688\n",
      "2017-12-14T10:48:04.198594: step 297, loss 0.646834, acc 0.625\n",
      "2017-12-14T10:48:05.798180: step 298, loss 0.696107, acc 0.632812\n",
      "2017-12-14T10:48:07.394491: step 299, loss 0.724437, acc 0.640625\n",
      "2017-12-14T10:48:08.882158: step 300, loss 0.744238, acc 0.650406\n",
      "\n",
      "Evaluation:\n",
      "2017-12-14T10:48:12.354567: step 300, loss 0.594543, acc 0.672915\n",
      "\n",
      "Saved model checkpoint to /home/hoang/Documents/svn/reposn/code/text-classification/runs/1513244435/checkpoints/model-300\n",
      "\n",
      "2017-12-14T10:48:13.817557: step 301, loss 0.647117, acc 0.664062\n",
      "2017-12-14T10:48:15.356810: step 302, loss 0.475742, acc 0.75\n",
      "2017-12-14T10:48:16.913880: step 303, loss 0.606397, acc 0.710938\n",
      "2017-12-14T10:48:18.473950: step 304, loss 0.585762, acc 0.71875\n",
      "2017-12-14T10:48:19.841806: step 305, loss 0.651303, acc 0.703125\n",
      "2017-12-14T10:48:21.273345: step 306, loss 0.623421, acc 0.648438\n",
      "2017-12-14T10:48:22.654304: step 307, loss 0.50941, acc 0.734375\n",
      "2017-12-14T10:48:24.013327: step 308, loss 0.58155, acc 0.757812\n",
      "2017-12-14T10:48:25.566574: step 309, loss 0.539567, acc 0.78125\n",
      "2017-12-14T10:48:27.106174: step 310, loss 0.519296, acc 0.734375\n",
      "2017-12-14T10:48:28.511017: step 311, loss 0.599158, acc 0.6875\n",
      "2017-12-14T10:48:29.902663: step 312, loss 0.585569, acc 0.71875\n",
      "2017-12-14T10:48:31.436298: step 313, loss 0.544794, acc 0.703125\n",
      "2017-12-14T10:48:32.828806: step 314, loss 0.486765, acc 0.757812\n",
      "2017-12-14T10:48:34.274402: step 315, loss 0.493117, acc 0.765625\n",
      "2017-12-14T10:48:35.876244: step 316, loss 0.537648, acc 0.75\n",
      "2017-12-14T10:48:37.404878: step 317, loss 0.641729, acc 0.664062\n",
      "2017-12-14T10:48:38.792107: step 318, loss 0.648534, acc 0.6875\n",
      "2017-12-14T10:48:40.130182: step 319, loss 0.563628, acc 0.695312\n",
      "2017-12-14T10:48:41.520613: step 320, loss 0.546876, acc 0.710938\n",
      "2017-12-14T10:48:42.882733: step 321, loss 0.592182, acc 0.65625\n",
      "2017-12-14T10:48:44.307325: step 322, loss 0.619537, acc 0.671875\n",
      "2017-12-14T10:48:45.865796: step 323, loss 0.509795, acc 0.742188\n",
      "2017-12-14T10:48:47.442781: step 324, loss 0.587573, acc 0.703125\n",
      "2017-12-14T10:48:48.949357: step 325, loss 0.658118, acc 0.6875\n",
      "2017-12-14T10:48:50.354016: step 326, loss 0.680371, acc 0.664062\n",
      "2017-12-14T10:48:51.744185: step 327, loss 0.607786, acc 0.679688\n",
      "2017-12-14T10:48:53.129713: step 328, loss 0.543025, acc 0.695312\n",
      "2017-12-14T10:48:54.589706: step 329, loss 0.577518, acc 0.757812\n",
      "2017-12-14T10:48:56.139770: step 330, loss 0.558717, acc 0.71875\n",
      "2017-12-14T10:48:57.662389: step 331, loss 0.589431, acc 0.71875\n",
      "2017-12-14T10:48:59.143779: step 332, loss 0.558931, acc 0.71875\n",
      "2017-12-14T10:49:00.540968: step 333, loss 0.546272, acc 0.773438\n",
      "2017-12-14T10:49:01.935860: step 334, loss 0.658729, acc 0.664062\n",
      "2017-12-14T10:49:03.311952: step 335, loss 0.62019, acc 0.664062\n",
      "2017-12-14T10:49:04.786498: step 336, loss 0.644676, acc 0.679688\n",
      "2017-12-14T10:49:06.384234: step 337, loss 0.545989, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-14T10:49:07.894646: step 338, loss 0.554529, acc 0.710938\n",
      "2017-12-14T10:49:09.289390: step 339, loss 0.614212, acc 0.671875\n",
      "2017-12-14T10:49:10.672654: step 340, loss 0.63254, acc 0.695312\n",
      "2017-12-14T10:49:12.105598: step 341, loss 0.635943, acc 0.6875\n",
      "2017-12-14T10:49:13.481069: step 342, loss 0.556423, acc 0.671875\n",
      "2017-12-14T10:49:15.005300: step 343, loss 0.616148, acc 0.757812\n",
      "2017-12-14T10:49:16.528108: step 344, loss 0.528351, acc 0.734375\n",
      "2017-12-14T10:49:18.074791: step 345, loss 0.656053, acc 0.648438\n",
      "2017-12-14T10:49:19.403520: step 346, loss 0.603632, acc 0.664062\n",
      "2017-12-14T10:49:20.778068: step 347, loss 0.517037, acc 0.757812\n",
      "2017-12-14T10:49:22.290019: step 348, loss 0.619751, acc 0.6875\n",
      "2017-12-14T10:49:23.658793: step 349, loss 0.582411, acc 0.703125\n",
      "2017-12-14T10:49:25.221096: step 350, loss 0.646569, acc 0.671875\n",
      "2017-12-14T10:49:26.904471: step 351, loss 0.450287, acc 0.789062\n",
      "2017-12-14T10:49:28.370430: step 352, loss 0.560632, acc 0.734375\n",
      "2017-12-14T10:49:29.753363: step 353, loss 0.582064, acc 0.703125\n",
      "2017-12-14T10:49:31.188223: step 354, loss 0.655913, acc 0.640625\n",
      "2017-12-14T10:49:32.543376: step 355, loss 0.665215, acc 0.6875\n",
      "2017-12-14T10:49:34.124197: step 356, loss 0.688173, acc 0.601562\n",
      "2017-12-14T10:49:35.730464: step 357, loss 0.551021, acc 0.703125\n",
      "2017-12-14T10:49:37.268938: step 358, loss 0.514041, acc 0.75\n",
      "2017-12-14T10:49:38.651196: step 359, loss 0.517916, acc 0.710938\n",
      "2017-12-14T10:49:40.060128: step 360, loss 0.637892, acc 0.695312\n",
      "2017-12-14T10:49:41.416594: step 361, loss 0.593455, acc 0.6875\n",
      "2017-12-14T10:49:42.875270: step 362, loss 0.549072, acc 0.679688\n",
      "2017-12-14T10:49:44.290539: step 363, loss 0.656841, acc 0.65625\n",
      "2017-12-14T10:49:45.925283: step 364, loss 0.595137, acc 0.6875\n",
      "2017-12-14T10:49:47.387673: step 365, loss 0.491837, acc 0.757812\n",
      "2017-12-14T10:49:48.825645: step 366, loss 0.514877, acc 0.765625\n",
      "2017-12-14T10:49:50.207360: step 367, loss 0.651375, acc 0.6875\n",
      "2017-12-14T10:49:51.538795: step 368, loss 0.598796, acc 0.703125\n",
      "2017-12-14T10:49:52.946853: step 369, loss 0.599736, acc 0.710938\n",
      "2017-12-14T10:49:54.399667: step 370, loss 0.526722, acc 0.773438\n",
      "2017-12-14T10:49:55.963820: step 371, loss 0.495659, acc 0.757812\n",
      "2017-12-14T10:49:57.545626: step 372, loss 0.609521, acc 0.664062\n",
      "2017-12-14T10:49:58.963150: step 373, loss 0.658486, acc 0.65625\n",
      "2017-12-14T10:50:00.367992: step 374, loss 0.502958, acc 0.726562\n",
      "2017-12-14T10:50:01.677819: step 375, loss 0.601847, acc 0.691057\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def train():\n",
    "\n",
    "      with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        \n",
    "#         sess = tf.Session(config=session_conf)\n",
    "#         with sess.as_default(): #\n",
    "        with tf.Session(config=session_conf) as sess:\n",
    "            #init model\n",
    "            cnn = Word_CNN_Model(\n",
    "                input_dim=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            ### Define training procedure\n",
    "            #global step value to be used throughout training and testing\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            #define Adam optim\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "            #compute gradients\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss) #cnn.loss is a property defined in the scope \"loss\"\n",
    "            #apply above gradients \n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step) #be used in train_step()\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    #Adding a histogram summary to visualize data's distribution in TensorBoar\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g) #(node_name, values)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries) # merge all grad summaries\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time())) \n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp)) #out_dir=./runs/timestamp\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss) # a summary to monitor cost tensor\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy) #a summary to monitor accuracy\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged]) #will be used in train_step()\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Test summaries\n",
    "            test_summary_op = tf.summary.merge([loss_summary, acc_summary])# be used in test_step()\n",
    "            test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "            test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\")) #./runs/timestamp/checkpoints\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\") #prefix of filename\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir) # default, TF assumes checkpoint dir exists\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Save vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\")) # ./runs/timestamp/vocab\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.keep_prob: FLAGS.keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict) # run all summaries with the input=feed_dict\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                #display info: 2017-12-08T16:00:45.606711: step 8, loss 3.45242, acc 0.5\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                #add summary\n",
    "                train_summary_writer.add_summary(summaries, step) #(buffer, global step value)\n",
    "\n",
    "            def test_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a test set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, test_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy)) \n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches (from preprocessing part above)\n",
    "            batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            \n",
    "            # Training loop. Note, nb of batchs/iterations = len(data)/batch_size\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step) #get the current step of training\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    \n",
    "    \n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 5331  5332  5333 ..., 10659 10660 10661] | test: [   0    1    2 ..., 5328 5329 5330]\n",
      "3 (?, 1, 1, 384) (?, 384)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/W:0/grad/hist is illegal; using conv_maxpool_block_3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/W:0/grad/sparsity is illegal; using conv_maxpool_block_3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/b_0:0/grad/hist is illegal; using conv_maxpool_block_3/b_0_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_3/b_0:0/grad/sparsity is illegal; using conv_maxpool_block_3/b_0_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/W:0/grad/hist is illegal; using conv_maxpool_block_4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/W:0/grad/sparsity is illegal; using conv_maxpool_block_4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/b_1:0/grad/hist is illegal; using conv_maxpool_block_4/b_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_4/b_1:0/grad/sparsity is illegal; using conv_maxpool_block_4/b_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/W:0/grad/hist is illegal; using conv_maxpool_block_5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/W:0/grad/sparsity is illegal; using conv_maxpool_block_5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/b_2:0/grad/hist is illegal; using conv_maxpool_block_5/b_2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv_maxpool_block_5/b_2:0/grad/sparsity is illegal; using conv_maxpool_block_5/b_2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name b:0/grad/hist is illegal; using b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name b:0/grad/sparsity is illegal; using b_0/grad/sparsity instead.\n",
      "Writing to /home/hoang/Documents/svn/reposn/code/text-classification/runs/1513184876\n",
      "\n",
      "2017-12-13T18:07:58.587867: step 1, loss 1.49842, acc 0.59375\n",
      "2017-12-13T18:08:00.074420: step 2, loss 0.458429, acc 0.804688\n",
      "2017-12-13T18:08:01.492776: step 3, loss 0.24222, acc 0.921875\n",
      "2017-12-13T18:08:02.992809: step 4, loss 0.0386387, acc 0.976562\n",
      "2017-12-13T18:08:04.478560: step 5, loss 0.00156417, acc 1\n",
      "2017-12-13T18:08:06.121211: step 6, loss 0.00115607, acc 1\n",
      "2017-12-13T18:08:07.634743: step 7, loss 8.2692e-05, acc 1\n",
      "2017-12-13T18:08:09.089677: step 8, loss 0.000424671, acc 1\n",
      "2017-12-13T18:08:10.550041: step 9, loss 2.73234e-05, acc 1\n",
      "2017-12-13T18:08:12.000617: step 10, loss 7.5502e-05, acc 1\n",
      "2017-12-13T18:08:13.383286: step 11, loss 4.01835e-05, acc 1\n",
      "2017-12-13T18:08:14.801073: step 12, loss 2.13438e-06, acc 1\n",
      "2017-12-13T18:08:16.199861: step 13, loss 4.48822e-06, acc 1\n",
      "2017-12-13T18:08:17.613664: step 14, loss 2.16996e-07, acc 1\n",
      "2017-12-13T18:08:19.071931: step 15, loss 1.89058e-07, acc 1\n",
      "2017-12-13T18:08:20.789930: step 16, loss 1.08033e-07, acc 1\n",
      "2017-12-13T18:08:22.317542: step 17, loss 4.19094e-08, acc 1\n",
      "2017-12-13T18:08:23.702119: step 18, loss 1.75087e-07, acc 1\n",
      "2017-12-13T18:08:25.046340: step 19, loss 8.94065e-08, acc 1\n",
      "2017-12-13T18:08:26.473384: step 20, loss 9.31322e-09, acc 1\n",
      "2017-12-13T18:08:27.813784: step 21, loss 2.79397e-09, acc 1\n",
      "2017-12-13T18:08:29.314375: step 22, loss 2.88709e-08, acc 1\n",
      "2017-12-13T18:08:30.821234: step 23, loss 2.79397e-09, acc 1\n",
      "2017-12-13T18:08:32.242531: step 24, loss 0, acc 1\n",
      "2017-12-13T18:08:33.651883: step 25, loss 0, acc 1\n",
      "2017-12-13T18:08:35.122009: step 26, loss 0, acc 1\n",
      "2017-12-13T18:08:36.521177: step 27, loss 0, acc 1\n",
      "2017-12-13T18:08:37.864523: step 28, loss 4.65661e-09, acc 1\n",
      "2017-12-13T18:08:39.275585: step 29, loss 0, acc 1\n",
      "2017-12-13T18:08:40.596375: step 30, loss 0, acc 1\n",
      "2017-12-13T18:08:41.987532: step 31, loss 1.86264e-09, acc 1\n",
      "2017-12-13T18:08:43.264265: step 32, loss 9.31323e-10, acc 1\n",
      "2017-12-13T18:08:44.638433: step 33, loss 1.86265e-09, acc 1\n",
      "2017-12-13T18:08:45.988240: step 34, loss 0, acc 1\n",
      "2017-12-13T18:08:47.305497: step 35, loss 2.79397e-09, acc 1\n",
      "2017-12-13T18:08:48.581034: step 36, loss 9.31323e-10, acc 1\n",
      "2017-12-13T18:08:49.981857: step 37, loss 2.04891e-08, acc 1\n",
      "2017-12-13T18:08:51.384734: step 38, loss 2.79397e-09, acc 1\n",
      "2017-12-13T18:08:52.792289: step 39, loss 0, acc 1\n",
      "2017-12-13T18:08:54.176174: step 40, loss 0, acc 1\n",
      "2017-12-13T18:08:55.616486: step 41, loss 3.72529e-09, acc 1\n",
      "2017-12-13T18:08:56.522603: step 42, loss 1.43626e-09, acc 1\n",
      "2017-12-13T18:08:57.898664: step 43, loss 9.31323e-10, acc 1\n",
      "2017-12-13T18:08:59.294965: step 44, loss 9.31323e-10, acc 1\n",
      "2017-12-13T18:09:00.692722: step 45, loss 0, acc 1\n",
      "2017-12-13T18:09:02.245158: step 46, loss 9.31323e-10, acc 1\n",
      "2017-12-13T18:09:03.613903: step 47, loss 1.22934e-07, acc 1\n",
      "2017-12-13T18:09:05.180111: step 48, loss 4.65661e-09, acc 1\n",
      "2017-12-13T18:09:06.698455: step 49, loss 9.31323e-10, acc 1\n",
      "2017-12-13T18:09:08.202389: step 50, loss 3.07336e-08, acc 1\n",
      "2017-12-13T18:09:09.870578: step 51, loss 0, acc 1\n",
      "2017-12-13T18:09:11.372221: step 52, loss 0, acc 1\n",
      "2017-12-13T18:09:12.837384: step 53, loss 0, acc 1\n",
      "2017-12-13T18:09:14.511938: step 54, loss 0, acc 1\n",
      "2017-12-13T18:09:16.309578: step 55, loss 9.31323e-10, acc 1\n"
     ]
    }
   ],
   "source": [
    "####### Training with CV #######\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=FLAGS.num_folds, shuffle=False)\n",
    "for train_indices, test_indices in k_fold.split(x_text):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    \n",
    "def train_kfold():\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        \n",
    "        for train_indices, test_indices in k_fold.split(x_text):\n",
    "                print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "                x_train = x[train_indices]\n",
    "                y_train = y[train_indices]\n",
    "                x_test = x[test_indices]\n",
    "                y_test = y[test_indices]\n",
    "        with tf.Session(config=session_conf) as sess:\n",
    "            k_fold = KFold(n_splits=FLAGS.num_folds, shuffle=False)\n",
    "            \n",
    " \n",
    "                #init model\n",
    "                cnn = Word_CNN_Model(\n",
    "                    input_dim=x_train.shape[1],\n",
    "                    num_classes=y_train.shape[1],\n",
    "                    vocab_size=len(vocab_processor.vocabulary_),\n",
    "                    embedding_size=FLAGS.embedding_dim,\n",
    "                    filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                    num_filters=FLAGS.num_filters,\n",
    "                    l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "                ### Define training procedure\n",
    "                #global step value to be used throughout training and testing\n",
    "                global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "                #define Adam optim\n",
    "                optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "                #compute gradients\n",
    "                grads_and_vars = optimizer.compute_gradients(cnn.loss) #cnn.loss is a property defined in the scope \"loss\"\n",
    "                #apply above gradients \n",
    "                train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step) #be used in train_step()\n",
    "\n",
    "                # Keep track of gradient values and sparsity (optional)\n",
    "                grad_summaries = []\n",
    "                for g, v in grads_and_vars:\n",
    "                    if g is not None:\n",
    "                        #Adding a histogram summary to visualize data's distribution in TensorBoar\n",
    "                        grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g) #(node_name, values)\n",
    "                        sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                        grad_summaries.append(grad_hist_summary)\n",
    "                        grad_summaries.append(sparsity_summary)\n",
    "                grad_summaries_merged = tf.summary.merge(grad_summaries) # merge all grad summaries\n",
    "\n",
    "                # Output directory for models and summaries\n",
    "                timestamp = str(int(time.time())) \n",
    "                out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp)) #out_dir=./runs/timestamp\n",
    "                print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "                # Summaries for loss and accuracy\n",
    "                loss_summary = tf.summary.scalar(\"loss\", cnn.loss) # a summary to monitor cost tensor\n",
    "                acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy) #a summary to monitor accuracy\n",
    "\n",
    "                # Train Summaries\n",
    "                train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged]) #will be used in train_step()\n",
    "                train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "                train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "                # Test summaries\n",
    "                test_summary_op = tf.summary.merge([loss_summary, acc_summary])# be used in test_step()\n",
    "                test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
    "                test_summary_writer = tf.summary.FileWriter(test_summary_dir, sess.graph)\n",
    "\n",
    "                # Checkpoint directory\n",
    "                checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\")) #./runs/timestamp/checkpoints\n",
    "                checkpoint_prefix = os.path.join(checkpoint_dir, \"model\") #prefix of filename\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir) # default, TF assumes checkpoint dir exists\n",
    "                saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "                # Save vocabulary\n",
    "                vocab_processor.save(os.path.join(out_dir, \"vocab\")) # ./runs/timestamp/vocab\n",
    "\n",
    "                # Initialize all variables\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                def train_step(x_batch, y_batch):\n",
    "                    \"\"\"\n",
    "                    A single training step\n",
    "                    \"\"\"\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch,\n",
    "                      cnn.input_y: y_batch,\n",
    "                      cnn.keep_prob: FLAGS.keep_prob\n",
    "                    }\n",
    "                    _, step, summaries, loss, accuracy = sess.run(\n",
    "                        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                        feed_dict) # run all summaries with the input=feed_dict\n",
    "\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "                    #display info: 2017-12-08T16:00:45.606711: step 8, loss 3.45242, acc 0.5\n",
    "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                    #add summary\n",
    "                    train_summary_writer.add_summary(summaries, step) #(buffer, global step value)\n",
    "\n",
    "                def test_step(x_batch, y_batch, writer=None):\n",
    "                    \"\"\"\n",
    "                    Evaluates model on a test set\n",
    "                    \"\"\"\n",
    "                    feed_dict = {\n",
    "                      cnn.input_x: x_batch,\n",
    "                      cnn.input_y: y_batch,\n",
    "                      cnn.keep_prob: 1.0\n",
    "                    }\n",
    "                    step, summaries, loss, accuracy = sess.run(\n",
    "                        [global_step, test_summary_op, cnn.loss, cnn.accuracy],\n",
    "                        feed_dict)\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy)) \n",
    "                    if writer:\n",
    "                        writer.add_summary(summaries, step)\n",
    "\n",
    "                # Generate batches (from preprocessing part above)\n",
    "                batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "                # Training loop. Note, nb of batchs/iterations = len(data)/batch_size\n",
    "                for batch in batches:\n",
    "                    x_batch, y_batch = zip(*batch)\n",
    "                    train_step(x_batch, y_batch)\n",
    "                    current_step = tf.train.global_step(sess, global_step) #get the current step of training\n",
    "                    if current_step % FLAGS.evaluate_every == 0:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        test_step(x_test, y_test, writer=test_summary_writer)\n",
    "                        print(\"\")\n",
    "                    if current_step % FLAGS.checkpoint_every == 0:\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    \n",
    "train_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "\n",
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])\n",
    "\n",
    "input1 = tf.random_normal([1,10,10,32])\n",
    "input2 = tf.random_normal([1,20,20,32])\n",
    "\n",
    "x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n",
    "x = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])  # This fails.\n",
    "\n",
    "with tf.variable_scope(\"model\"):\n",
    "  output1 = my_image_filter(input1)\n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "  output2 = my_image_filter(input2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pdb\n",
    "\n",
    "def main():\n",
    "\n",
    "    ## !!! change this to test the different behaviors !!!\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(1e-3)                 # This one is working\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3, beta1=0.9, beta2=0.999999) # This one is not working\n",
    "    #optimizer = tf.train.AdagradOptimizer(1e-3)                         # This one is not working\n",
    "    #optimizer = tf.train.AdadeltaOptimizer(1e-3)                        # This one is not working\n",
    "\n",
    "    list_grads = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "\n",
    "        for i in range(2):\n",
    "            with tf.name_scope('%d' % i) as scope:\n",
    "                    W = tf.get_variable(name=\"filter\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n",
    "                    X = tf.get_variable(name=\"data\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n",
    "                    Y_ = tf.get_variable(name=\"out\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n",
    "                    Y = W+X\n",
    "                    loss =tf.reduce_mean(Y-Y_)\n",
    "                    grad = optimizer.compute_gradients(loss)\n",
    "                    list_grads.append(grad)\n",
    "\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    grads = list_grads[0] + list_grads[1]\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    op_train = optimizer.apply_gradients(grads)\n",
    "\n",
    "    init_global = tf.global_variables_initializer()\n",
    "    init_local =  tf.local_variables_initializer()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    _, sol = sess.run([op_train, loss])\n",
    "    print(str(sol))\n",
    "\n",
    "if (__name__ == '__main__'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TextCNN(sequence_length=200, num_classes=10, vocab_size=200, embedding_size=100, filter_sizes=[5],  num_filters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
